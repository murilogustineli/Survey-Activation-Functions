# Understanding Activation Functions for Deep Neural Networks
Repo for storing content on my survey paper about Activation Functions on Deep Neural Networks.


### Referenced papers
| Paper PDF | Link to paper |
| --------- | ------------- |
| [Agostinelli (2015): Learning Activation Functions to Improve Deep Neural Networks](https://github.com/murilogustineli/Survey-Activation-Functions/blob/main/papers/Agostinelli-2015-Learning%20Activation%20Functions%20to%20Improve%20Deep%20Neural%20Networks.pdf) | [arxiv.org](https://arxiv.org/abs/1412.6830) |
| [Apicella (2021): A survey on modern trainable activation functions](https://github.com/murilogustineli/Survey-Activation-Functions/blob/main/papers/Apicella-2021-A%20survey%20on%20modern%20trainable%20activation%20functions.pdf) | [arxiv.org](https://arxiv.org/abs/2005.00817) |
| [Datta (2020): A Survey on Activation Functions and their relation with Xavier and He Normal Initialization](https://github.com/murilogustineli/Survey-Activation-Functions/blob/main/papers/Datta-2020-A%20Survey%20on%20Activation%20Functions%20and%20their%20relation%20with%20Xavier%20and%20He%20Normal%20Initialization.pdf) | [arxiv.org](https://arxiv.org/abs/2004.06632) |
| [Glorot (2010): Understanding the difficulty of training deep feedforward neural networks](https://github.com/murilogustineli/Survey-Activation-Functions/blob/main/papers/Glorot-2010-Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks.pdf) | [proceedings.mlr.press](http://proceedings.mlr.press/v9/glorot10a) |
| [Gu (2018): Recent Advances in Convolutional Neural Networks](https://github.com/murilogustineli/Survey-Activation-Functions/blob/main/papers/Gu-2018-Recent%20Advances%20in%20Convolutional%20Neural%20Networks.pdf) | [sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S0031320317304120) |
| [Han (1995): The Influence of the Sigmoid Function Parameters on the Speed of Backpropagation Learning]() | [springer.com](https://link.springer.com/chapter/10.1007/3-540-59497-3_175#citeas) |
| [Hecht-Nielsen (1992): Theory of the Backpropagation Neural Network](https://github.com/murilogustineli/Survey-Activation-Functions/blob/main/papers/Hecht-Nielsen-1992-Theory%20of%20the%20Backpropagation%20Neural%20Network.pdf) | [sciencedirect.com](https://www.sciencedirect.com/science/article/pii/B9780127412528500108) |
| [Hornik (1989): Multilayer Feedforward Networks are Universal Approximators](https://github.com/murilogustineli/Survey-Activation-Functions/blob/main/papers/Hornik-1989-Multilayer%20Feedforward%20Networks%20are%20Universal%20Approximators.pdf) | [sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/0893608089900208) |
| [LeCun (1989): Handwritten Digit Recognition with a Back-Propagation Network](https://github.com/murilogustineli/Survey-Activation-Functions/blob/main/papers/LeCun-1989-Handwritten%20Digit%20Recognition%20with%20a%20Back-Propagation%20Network.pdf) | [proceedings.neurips.cc](https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf) |
| [LeCun (1998): Gradient-Based Learning Applied to Document Recognition](https://github.com/murilogustineli/Survey-Activation-Functions/blob/main/papers/LeCun-1998-Gradient-Based%20Learning%20Applied%20to%20Document%20Recognition.pdf) | [IEEE.org](https://ieeexplore.ieee.org/abstract/document/726791) |
| [LeCun (2012): Efficient BackProp](https://github.com/murilogustineli/Survey-Activation-Functions/blob/main/papers/LeCun-2012-Efficient%20BackProp.pdf) | [springer.com](https://link.springer.com/chapter/10.1007/978-3-642-35289-8_3) |
| [LeCun (2015): Deep Learning](https://github.com/murilogustineli/Survey-Activation-Functions/blob/main/papers/LeCun-2015-Deep%20Learning.pdf) | [nature.com](https://www.nature.com/articles/nature14539) |
| [Misra (2020): Mish: A Self Regularized Non-Monotonic Activation Function](https://github.com/murilogustineli/Survey-Activation-Functions/blob/main/papers/Misra-2020-Mish%3B%20A%20Self%20Regularized%20Non-Monotonic%20Activation%20Function.pdf) | [arxiv.org](https://arxiv.org/abs/1908.08681) |
| [Noel (2021): Growing Cosine Unit: A Novel Oscillatory Activation Function That Can Speedup Training and Reduce Parameters in Convolutional Neural Networks](https://github.com/murilogustineli/Survey-Activation-Functions/blob/main/papers/Noel-2021-Growing%20Cosine%20Unit%3B%20A%20Novel%20Oscillatory%20Activation%20Function%20That%20Can%20Speedup%20Training%20and%20Reduce%20Parameters%20in%20CNN.pdf) | [arxiv.org](https://arxiv.org/abs/2108.12943) |
| [Noel (2022): Biologically Inspired Oscillating Activation Functions Can Bridge the Performance Gap between Biological and Artificial Neurons](https://github.com/murilogustineli/Survey-Activation-Functions/blob/main/papers/Noel-2022-Biologically%20Inspired%20Oscillating%20Activation%20Functions%20Can%20Bridge%20the%20Performance%20Gap%20between%20Biological%20and%20Artificial%20Neurons.pdf) | [arxiv.org](https://arxiv.org/abs/2111.04020) |
| [Ramachandran (2017): Searching for Activation Functions](https://github.com/murilogustineli/Survey-Activation-Functions/blob/main/papers/Ramachandran-2017-Searching%20for%20Activation%20Functions.pdf) | [arxiv.org](https://arxiv.org/abs/1710.05941) |
| [Russakovsky (2014): ImageNet Large Scale Visual Recognition Challenge](https://github.com/murilogustineli/Survey-Activation-Functions/blob/main/papers/Russakovsky-2014-ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge.pdf) | [arxiv.org](https://arxiv.org/abs/1409.0575) |
| [Schmidhuber (2014): Deep Learning in Neural Networks: An Overview](https://github.com/murilogustineli/Survey-Activation-Functions/blob/main/papers/Schmidhuber-2014-Deep%20Learning%20in%20Neural%20Networks%3B%20An%20Overview.pdf) | [arxiv.org](https://arxiv.org/abs/1404.7828) |
